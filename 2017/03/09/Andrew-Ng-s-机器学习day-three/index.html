<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    
    <title>Andrew Ng&#39;s 机器学习Day Three | blaceman</title>
    <meta name="renderer" content="webkit">
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

    <meta name="description" content="黄昏,一天最美丽的谢幕~">

    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="Andrew Ng&#39;s 机器学习Day Three | blaceman">
    <meta name="twitter:description" content="黄昏,一天最美丽的谢幕~">

    <meta property="og:type" content="article">
    <meta property="og:title" content="Andrew Ng&#39;s 机器学习Day Three | blaceman">
    <meta property="og:description" content="黄昏,一天最美丽的谢幕~">

    
    <meta name="author" content="blaceman">
    
    <link rel="stylesheet" href="../../../../css/vno.css">
    <link rel="stylesheet" href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css">

    
    <link rel="icon" href="/images/avatar-small.png">
    

    <meta name="generator" content="hexo"/>
    
    <link rel="alternate" type="application/rss+xml" title="blaceman" href="/atom.xml">
    

    <link rel="canonical" href="https://blaceman.github.io/2017/03/09/Andrew-Ng-s-机器学习day-three/"/>

    
      
</head>

<body class="home-template no-js">
    <script src="//cdn.bootcss.com/jquery/2.1.4/jquery.min.js"></script>
    <script src="../../../../js/main.js"></script>
    <span class="mobile btn-mobile-menu">
        <i class="fa fa-list btn-mobile-menu__icon"></i>
        <i class="fa fa-angle-up btn-mobile-close__icon hidden"></i>
    </span>

    
<header class="panel-cover panel-cover--collapsed" style="background-image: url(/images/IMG_0364.jpg)">
  <div class="panel-main">
    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">

        <a href="/" title="前往 blaceman 的主页"><img src="/images/IMG_0355.JPG" width="80" alt="blaceman logo" class="panel-cover__logo logo" /></a>
        <h1 class="panel-cover__title panel-title"><a href="/" title="link to homepage for blaceman">blaceman</a></h1>
        
        <span class="panel-cover__subtitle panel-subtitle">懵懂 前进 憧憬</span>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">黄昏,一天最美丽的谢幕~</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />

        <div class="navigation-wrapper">
          <div>
          <nav class="cover-navigation cover-navigation--primary">
            <ul class="navigation">
              <li class="navigation__item"><a href="/#blog" title="访问博客" class="blog-button">博客</a></li>
            
              <li class="navigation__item"><a href="/favourite">黄金屋</a></li>
            
              <li class="navigation__item"><a href="/about">关于我</a></li>
            
            </ul>
          </nav>
          </div>
          <div>
          <nav class="cover-navigation navigation--social">
  <ul class="navigation">

  <!-- Weibo-->
  
  <li class="navigation__item">
    <a href="https://weibo.com/blaceman" title="我的微博" target="_blank">
      <i class='social fa fa-weibo'></i>
      <span class="label">Weibo</span>
    </a>
  </li> 


  <!-- Github -->
  
  <li class="navigation__item">
    <a href="https://github.com/blaceman" title="查看我的GitHub主页" target="_blank">
      <i class='social fa fa-github'></i>
      <span class="label">Github</span>
    </a>
  </li>


<!-- Stack Overflow -->
        

  <!-- Google Plus -->
  

<!-- Facebook -->

  <li class="navigation__item">
    <a href="https://facebook.com/blacemen" title="上Facebook找我" target="_blank">
      <i class='social fa fa-facebook'></i>
      <span class="label">Facebook</span>
    </a>
  </li>

  
<!-- Twitter -->

  

  <li class="navigation__item">
    <a href="/atom.xml" title="RSS" target="_blank">
      <i class='social fa fa-rss'></i>
      <span class="label">RSS</span>
    </a>
  </li>



  </ul>
</nav>

          </div>
        </div>

      </div>

    </div>

    <div class="panel-cover--overlay cover-purple"></div>
  </div>
</header>

    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            <article class="post-container post-container--single">

  <header class="post-header">
    <div class="post-meta">
      <time datetime="2017-03-09T09:16:54.000Z" class="post-list__meta--date date">2017-03-09</time> &#8226; <span class="post-meta__tags tags">于&nbsp; </span>
      <span class="page-pv">
      &nbsp;阅读&nbsp;<span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span>
      </span> 
   
    </div>
    <h1 class="post-title">Andrew Ng&#39;s 机器学习Day Three</h1>

  </header>

  <section class="post">
    <h3 id="Paramater-Learning-参数学习"><a href="#Paramater-Learning-参数学习" class="headerlink" title="Paramater Learning(参数学习)"></a>Paramater Learning(参数学习)</h3><h4 id="1-Gradient-Descent-梯度下降"><a href="#1-Gradient-Descent-梯度下降" class="headerlink" title="1.Gradient Descent(梯度下降)"></a>1.Gradient Descent(梯度下降)</h4><h5 id="假定我们有一个hypothesis函数-我们有一个方法测量合适的数据-现在我们需要估计hypothesis函数的参数-这时梯度下降派上用场"><a href="#假定我们有一个hypothesis函数-我们有一个方法测量合适的数据-现在我们需要估计hypothesis函数的参数-这时梯度下降派上用场" class="headerlink" title="假定我们有一个hypothesis函数,我们有一个方法测量合适的数据,现在我们需要估计hypothesis函数的参数,这时梯度下降派上用场"></a>假定我们有一个hypothesis函数,我们有一个方法测量合适的数据,现在我们需要估计hypothesis函数的参数,这时<code>梯度下降</code>派上用场</h5><h5 id="假设我们要画出基于θ0和θ1的假设函数-hypothesis-实际上我们把代价函数绘制为参数估值函数-我们不是绘制x和y本身，而是我们的假设函数的参数范围和由选择特定参数集所产生的代价"><a href="#假设我们要画出基于θ0和θ1的假设函数-hypothesis-实际上我们把代价函数绘制为参数估值函数-我们不是绘制x和y本身，而是我们的假设函数的参数范围和由选择特定参数集所产生的代价" class="headerlink" title="假设我们要画出基于θ0和θ1的假设函数(hypothesis)(实际上我们把代价函数绘制为参数估值函数) 我们不是绘制x和y本身，而是我们的假设函数的参数范围和由选择特定参数集所产生的代价"></a>假设我们要画出基于θ0和θ1的假设函数(hypothesis)(实际上我们把代价函数绘制为参数估值函数) 我们不是绘制x和y本身，而是我们的假设函数的参数范围和由选择特定参数集所产生的代价</h5><h5 id="我们把θ0放在x轴上，把θ1放在y轴上，成本函数放在垂直z轴上。-我们的图上的点将是使用具有那些特定θ参数的我们的假设的代价函数的结果。-下图描绘了这样的设置"><a href="#我们把θ0放在x轴上，把θ1放在y轴上，成本函数放在垂直z轴上。-我们的图上的点将是使用具有那些特定θ参数的我们的假设的代价函数的结果。-下图描绘了这样的设置" class="headerlink" title="我们把θ0放在x轴上，把θ1放在y轴上，成本函数放在垂直z轴上。 我们的图上的点将是使用具有那些特定θ参数的我们的假设的代价函数的结果。 下图描绘了这样的设置"></a>我们把θ0放在x轴上，把θ1放在y轴上，成本函数放在垂直z轴上。 我们的图上的点将是使用具有那些特定θ参数的我们的假设的代价函数的结果。 下图描绘了这样的设置</h5><p><img src="/2017/03/09/Andrew-Ng-s-机器学习day-three/1.png" alt="1"></p>
<h5 id="我们知道，当我们的成本函数在图中的凹坑的最底部时，即当它的值是最小值时，我们已经成功。红色箭头显示图中的最小点。"><a href="#我们知道，当我们的成本函数在图中的凹坑的最底部时，即当它的值是最小值时，我们已经成功。红色箭头显示图中的最小点。" class="headerlink" title="我们知道，当我们的成本函数在图中的凹坑的最底部时，即当它的值是最小值时，我们已经成功。红色箭头显示图中的最小点。"></a>我们知道，当我们的成本函数在图中的凹坑的最底部时，即当它的值是最小值时，我们已经成功。红色箭头显示图中的最小点。</h5><h5 id="我们这样做的方式是通过我们的代价函数的导数（一个函数的切线）。切线的斜率是在该点的导数，它将给我们移动的方向。我们在具有最陡下降的方向上逐步降低代价函数。每个步骤的大小由参数α确定，其被称为学习速率。"><a href="#我们这样做的方式是通过我们的代价函数的导数（一个函数的切线）。切线的斜率是在该点的导数，它将给我们移动的方向。我们在具有最陡下降的方向上逐步降低代价函数。每个步骤的大小由参数α确定，其被称为学习速率。" class="headerlink" title="我们这样做的方式是通过我们的代价函数的导数（一个函数的切线）。切线的斜率是在该点的导数，它将给我们移动的方向。我们在具有最陡下降的方向上逐步降低代价函数。每个步骤的大小由参数α确定，其被称为学习速率。"></a>我们这样做的方式是通过我们的代价函数的导数（一个函数的切线）。切线的斜率是在该点的导数，它将给我们移动的方向。我们在具有最陡下降的方向上逐步降低代价函数。每个步骤的大小由参数α确定，其被称为学习速率。</h5><h5 id="例如，上图中每个“星”之间的距离表示由我们的参数α确定的步长。较小的α将导致较小的阶跃，较大的α导致较大的阶跃。步进的方向由J（θ0，θ1）的偏导数确定。根据图表上的起始位置，可以在不同的点结束。上图显示了两个不同的起点，分别位于两个不同的地方。"><a href="#例如，上图中每个“星”之间的距离表示由我们的参数α确定的步长。较小的α将导致较小的阶跃，较大的α导致较大的阶跃。步进的方向由J（θ0，θ1）的偏导数确定。根据图表上的起始位置，可以在不同的点结束。上图显示了两个不同的起点，分别位于两个不同的地方。" class="headerlink" title="例如，上图中每个“星”之间的距离表示由我们的参数α确定的步长。较小的α将导致较小的阶跃，较大的α导致较大的阶跃。步进的方向由J（θ0，θ1）的偏导数确定。根据图表上的起始位置，可以在不同的点结束。上图显示了两个不同的起点，分别位于两个不同的地方。"></a>例如，上图中每个“星”之间的距离表示由我们的参数α确定的步长。较小的α将导致较小的阶跃，较大的α导致较大的阶跃。步进的方向由J（θ0，θ1）的偏导数确定。根据图表上的起始位置，可以在不同的点结束。上图显示了两个不同的起点，分别位于两个不同的地方。</h5><h5 id="梯度下降算法是："><a href="#梯度下降算法是：" class="headerlink" title="梯度下降算法是："></a>梯度下降算法是：</h5><h5 id="重复直到收敛："><a href="#重复直到收敛：" class="headerlink" title="重复直到收敛："></a>重复直到收敛：</h5><h5 id="θj-θj-−-α-∂-∂θj-J-θ0-θ1"><a href="#θj-θj-−-α-∂-∂θj-J-θ0-θ1" class="headerlink" title="θj:= θj − α ∂/∂θj J(θ0,θ1)"></a>θj:= θj − α ∂/∂θj J(θ0,θ1)</h5><h5 id="j-0-1表示特征索引号。"><a href="#j-0-1表示特征索引号。" class="headerlink" title="j = 0,1表示特征索引号。"></a>j = 0,1表示特征索引号。</h5><h5 id="在每次迭代j时，应该同时更新参数θ1，θ2，…，θn。-在第j次迭代计算另一个参数之前更新特定参数将导致错误的实现"><a href="#在每次迭代j时，应该同时更新参数θ1，θ2，…，θn。-在第j次迭代计算另一个参数之前更新特定参数将导致错误的实现" class="headerlink" title="在每次迭代j时，应该同时更新参数θ1，θ2，…，θn。 在第j次迭代计算另一个参数之前更新特定参数将导致错误的实现"></a>在每次迭代j时，应该同时更新参数θ1，θ2，…，θn。 在第j次迭代计算另一个参数之前更新特定参数将导致错误的实现</h5><p><img src="/2017/03/09/Andrew-Ng-s-机器学习day-three/2.png" alt="2"></p>
<h4 id="2-Gradient-Descent-Intuition"><a href="#2-Gradient-Descent-Intuition" class="headerlink" title="2.Gradient Descent Intuition"></a>2.Gradient Descent Intuition</h4><h5 id="在这个视频中，我们探索了一个场景，其中我们使用一个参数θ1，并绘制其代价函数来实现梯度下降。-我们对单个参数的公式是："><a href="#在这个视频中，我们探索了一个场景，其中我们使用一个参数θ1，并绘制其代价函数来实现梯度下降。-我们对单个参数的公式是：" class="headerlink" title="在这个视频中，我们探索了一个场景，其中我们使用一个参数θ1，并绘制其代价函数来实现梯度下降。 我们对单个参数的公式是："></a>在这个视频中，我们探索了一个场景，其中我们使用一个参数θ1，并绘制其代价函数来实现梯度下降。 我们对单个参数的公式是：</h5><h5 id="重复直到收敛：θ1：-θ1-α-d-dθ1-J-θ1）"><a href="#重复直到收敛：θ1：-θ1-α-d-dθ1-J-θ1）" class="headerlink" title="重复直到收敛：θ1：=θ1 - α d/dθ1 J(θ1）"></a>重复直到收敛：θ1：=θ1 - α d/dθ1 J(θ1）</h5><h5 id="无论d-dθ1-J-θ1）的斜率符号如何，θ1最终收敛到其最小值。-下图表示当斜率为负时，θ1的值增加，而当其为正时，θ1的值减小。"><a href="#无论d-dθ1-J-θ1）的斜率符号如何，θ1最终收敛到其最小值。-下图表示当斜率为负时，θ1的值增加，而当其为正时，θ1的值减小。" class="headerlink" title="无论d/dθ1 J(θ1）的斜率符号如何，θ1最终收敛到其最小值。 下图表示当斜率为负时，θ1的值增加，而当其为正时，θ1的值减小。"></a>无论d/dθ1 J(θ1）的斜率符号如何，θ1最终收敛到其最小值。 下图表示当斜率为负时，θ1的值增加，而当其为正时，θ1的值减小。</h5><p><img src="/2017/03/09/Andrew-Ng-s-机器学习day-three/3.png" alt="3"></p>
<h5 id="另一方面，我们应该调整参数α，以确保梯度下降算法在合理的时间内收敛。-不收敛或太多的时间来获得最小值意味着我们的步长是错误的。"><a href="#另一方面，我们应该调整参数α，以确保梯度下降算法在合理的时间内收敛。-不收敛或太多的时间来获得最小值意味着我们的步长是错误的。" class="headerlink" title="另一方面，我们应该调整参数α，以确保梯度下降算法在合理的时间内收敛。 不收敛或太多的时间来获得最小值意味着我们的步长是错误的。"></a>另一方面，我们应该调整参数α，以确保梯度下降算法在合理的时间内收敛。 不收敛或太多的时间来获得最小值意味着我们的步长是错误的。</h5><p><img src="/2017/03/09/Andrew-Ng-s-机器学习day-three/4.png" alt="4"></p>
<h5 id="梯度下降如何以固定步长α收敛？"><a href="#梯度下降如何以固定步长α收敛？" class="headerlink" title="梯度下降如何以固定步长α收敛？"></a>梯度下降如何以固定步长α收敛？</h5><h5 id="收敛后面的直觉是当我们接近我们的凸函数的底部时，d-dθ1-J-θ1）接近0。-至少，导数将始终为0，因此我们得到"><a href="#收敛后面的直觉是当我们接近我们的凸函数的底部时，d-dθ1-J-θ1）接近0。-至少，导数将始终为0，因此我们得到" class="headerlink" title="收敛后面的直觉是当我们接近我们的凸函数的底部时，d/dθ1 J(θ1）接近0。 至少，导数将始终为0，因此我们得到:"></a>收敛后面的直觉是当我们接近我们的凸函数的底部时，d/dθ1 J(θ1）接近0。 至少，导数将始终为0，因此我们得到:</h5><h5 id="θ1-θ1−α∗0"><a href="#θ1-θ1−α∗0" class="headerlink" title="θ1:=θ1−α∗0"></a><code>θ1:=θ1−α∗0</code></h5><p><img src="/2017/03/09/Andrew-Ng-s-机器学习day-three/5.png" alt="5"></p>
<h4 id="Gradient-Descent-For-Linear-Regression"><a href="#Gradient-Descent-For-Linear-Regression" class="headerlink" title="Gradient Descent For Linear Regression"></a>Gradient Descent For Linear Regression</h4><h5 id="Note-At-6-15-“h-x-900-0-1x”-should-be-“h-x-900-0-1x”"><a href="#Note-At-6-15-“h-x-900-0-1x”-should-be-“h-x-900-0-1x”" class="headerlink" title="Note: [At 6:15 “h(x) = -900 - 0.1x” should be “h(x) = 900 - 0.1x”]"></a>Note: [At 6:15 “h(x) = -900 - 0.1x” should be “h(x) = 900 - 0.1x”]</h5><h5 id="当具体应用于线性回归的情况下，可以导出梯度下降方程的新形式。我们可以替换我们的实际代价函数和我们的实际假设函数，并将公式修改为："><a href="#当具体应用于线性回归的情况下，可以导出梯度下降方程的新形式。我们可以替换我们的实际代价函数和我们的实际假设函数，并将公式修改为：" class="headerlink" title="当具体应用于线性回归的情况下，可以导出梯度下降方程的新形式。我们可以替换我们的实际代价函数和我们的实际假设函数，并将公式修改为："></a>当具体应用于线性回归的情况下，可以导出梯度下降方程的新形式。我们可以替换我们的实际代价函数和我们的实际假设函数，并将公式修改为：</h5><p><img src="/2017/03/09/Andrew-Ng-s-机器学习day-three/6.png" alt="6"></p>
<h5 id="其中m是训练集的大小，θ0是将与θ1同时改变的常数，xi，yi是给定训练集（数据）的值"><a href="#其中m是训练集的大小，θ0是将与θ1同时改变的常数，xi，yi是给定训练集（数据）的值" class="headerlink" title="其中m是训练集的大小，θ0是将与θ1同时改变的常数，xi，yi是给定训练集（数据）的值"></a>其中m是训练集的大小，θ0是将与θ1同时改变的常数，xi，yi是给定训练集（数据）的值</h5><h5 id="注意，我们已经将θj的两种情况分离为θ0和θ1的单独方程-并且对于θ1，由于导数，我们在末端乘以xi。以下是单个示例的∂-∂θj-J-θ）的推导："><a href="#注意，我们已经将θj的两种情况分离为θ0和θ1的单独方程-并且对于θ1，由于导数，我们在末端乘以xi。以下是单个示例的∂-∂θj-J-θ）的推导：" class="headerlink" title="注意，我们已经将θj的两种情况分离为θ0和θ1的单独方程;并且对于θ1，由于导数，我们在末端乘以xi。以下是单个示例的∂/∂θj J(θ）的推导："></a>注意，我们已经将θj的两种情况分离为θ0和θ1的单独方程;并且对于θ1，由于导数，我们在末端乘以xi。以下是单个示例的∂/∂θj J(θ）的推导：</h5><p><img src="/2017/03/09/Andrew-Ng-s-机器学习day-three/7.png" alt="7"></p>
<h5 id="所有这一切的点是，如果我们开始猜测我们的假设，然后重复应用这些梯度下降方程，我们的假设将变得越来越准确。"><a href="#所有这一切的点是，如果我们开始猜测我们的假设，然后重复应用这些梯度下降方程，我们的假设将变得越来越准确。" class="headerlink" title="所有这一切的点是，如果我们开始猜测我们的假设，然后重复应用这些梯度下降方程，我们的假设将变得越来越准确。"></a>所有这一切的点是，如果我们开始猜测我们的假设，然后重复应用这些梯度下降方程，我们的假设将变得越来越准确。</h5><h5 id="因此，这是对原始成本函数J的简单梯度下降。该方法查看每个步骤的整个训练集中的每个示例，并且称为批量梯度下降。注意，虽然梯度下降一般易受局部最小值的影响，我们在这里提出的用于线性回归的优化问题只有一个全局的，没有其他局部的最优的-因此梯度下降总是收敛（假设学习速率α不太大）到全局最小值。实际上，J是凸二次函数。这里是梯度下降的例子，因为它运行以最小化二次函数。"><a href="#因此，这是对原始成本函数J的简单梯度下降。该方法查看每个步骤的整个训练集中的每个示例，并且称为批量梯度下降。注意，虽然梯度下降一般易受局部最小值的影响，我们在这里提出的用于线性回归的优化问题只有一个全局的，没有其他局部的最优的-因此梯度下降总是收敛（假设学习速率α不太大）到全局最小值。实际上，J是凸二次函数。这里是梯度下降的例子，因为它运行以最小化二次函数。" class="headerlink" title="因此，这是对原始成本函数J的简单梯度下降。该方法查看每个步骤的整个训练集中的每个示例，并且称为批量梯度下降。注意，虽然梯度下降一般易受局部最小值的影响，我们在这里提出的用于线性回归的优化问题只有一个全局的，没有其他局部的最优的;因此梯度下降总是收敛（假设学习速率α不太大）到全局最小值。实际上，J是凸二次函数。这里是梯度下降的例子，因为它运行以最小化二次函数。"></a>因此，这是对原始成本函数J的简单梯度下降。该方法查看每个步骤的整个训练集中的每个示例，并且称为批量梯度下降。注意，虽然梯度下降一般易受局部最小值的影响，我们在这里提出的用于线性回归的优化问题只有一个全局的，没有其他局部的最优的;因此梯度下降总是收敛（假设学习速率α不太大）到全局最小值。实际上，J是凸二次函数。这里是梯度下降的例子，因为它运行以最小化二次函数。</h5><p><img src="/2017/03/09/Andrew-Ng-s-机器学习day-three/8.png" alt="8"></p>
<h5 id="上面显示的椭圆是二次函数的轮廓。还示出了由梯度下降采取的轨迹，其在（48-30）处被初始化。图中的x（用直线连接）标记了梯度下降经过的连续值，因为它收敛到最小值。"><a href="#上面显示的椭圆是二次函数的轮廓。还示出了由梯度下降采取的轨迹，其在（48-30）处被初始化。图中的x（用直线连接）标记了梯度下降经过的连续值，因为它收敛到最小值。" class="headerlink" title="上面显示的椭圆是二次函数的轮廓。还示出了由梯度下降采取的轨迹，其在（48,30）处被初始化。图中的x（用直线连接）标记了梯度下降经过的连续值，因为它收敛到最小值。"></a>上面显示的椭圆是二次函数的轮廓。还示出了由梯度下降采取的轨迹，其在（48,30）处被初始化。图中的x（用直线连接）标记了梯度下降经过的连续值，因为它收敛到最小值。</h5>
  </section>

</article>
<section class="read-more">
           
    
               
            <div class="read-more-item">
                <span class="read-more-item-dim">最近的文章</span>
                <h2 class="post-list__post-title post-title"><a href="/2017/03/13/Andrew-Ng-s-Machine-learning-first-day-of-the-second-week/" title="Andrew Ng&#39;s Machine Learning First Day of the Second Week">Andrew Ng&#39;s Machine Learning First Day of the Second Week</a></h2>
                <p class="excerpt">
                
                
                &hellip;
                </p>
                <div class="post-list__meta"><time datetime="2017-03-13T07:23:49.000Z" class="post-list__meta--date date">2017-03-13</time> &#8226; <span class="post-list__meta--tags tags">于&nbsp;</span><a class="btn-border-small" href="/2017/03/13/Andrew-Ng-s-Machine-learning-first-day-of-the-second-week/">继续阅读</a></div>
                           
            </div>
        
        
               
            <div class="read-more-item">
                <span class="read-more-item-dim">更早的文章</span>
                <h2 class="post-list__post-title post-title"><a href="/2017/03/05/Andrew-Ng-s-机器学习day-one/" title="Andrew Ng&#39;s 机器学习Day One">Andrew Ng&#39;s 机器学习Day One</a></h2>
                <p class="excerpt">
                
                前言: 什么是机器学习? NG老师举了几个例子.谷歌网页搜索,脸书或苹果的照片程序认出你的朋友,电子邮件中过滤垃圾邮件的反垃圾邮件…有一种科学能让计算机学习却不需要好高深的程序机器学习定义: A computer program is said to learn from experience E 
                &hellip;
                </p>
                <div class="post-list__meta"><time datetime="2017-03-05T09:11:56.000Z" class="post-list__meta--date date">2017-03-05</time> &#8226; <span class="post-list__meta--tags tags">于&nbsp;</span><a class="btn-border-small" href="/2017/03/05/Andrew-Ng-s-机器学习day-one/">继续阅读</a></div>
                       
            </div>
        
     
   
   
  
</section>

            
<section class="post-comments">
  <!-- 多说评论框 start -->
  <div class="ds-thread" data-thread-key="https://blaceman.github.io/2017/03/09/Andrew-Ng-s-机器学习day-three/" data-title="Andrew Ng&#39;s 机器学习Day Three" data-url="https://blaceman.github.io/2017/03/09/Andrew-Ng-s-机器学习day-three/"></div>
  <!-- 多说评论框 end -->
  <!-- 多说公共JS代码 start (一个网页只需插入一次) -->
  <script type="text/javascript">
  var duoshuoQuery = {short_name:"blaceman"};
  (function() {
    var ds = document.createElement('script');
    ds.type = 'text/javascript';ds.async = true;
    ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
    ds.charset = 'UTF-8';
    (document.getElementsByTagName('head')[0] 
     || document.getElementsByTagName('body')[0]).appendChild(ds);
  })();
  </script>
<!-- 多说公共JS代码 end -->
</section>


            <footer class="footer">
    <span class="footer__copyright">
        本站点采用 <a href="http://creativecommons.org/licenses/by-nc-sa/4.0/">知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议</a>
    </span>
    <span class="footer__copyright">
        基于 <a href="http://hexo.io">Hexo</a> 搭建，感谢 <a href="https://pages.github.com/">GitHub Pages</a> 提供免费的托管服务
    </span>
    <span class="footer__copyright">
        &copy; 2017 - 本站由 <a href="/">@Monniya</a> 创建,
        使用 <a href="https://github.com/monniya/hexo-theme-new-vno ">hexo-theme-new-vno</a> 主题,
        修改自 <a href="https://github.com/lenbo-ma/hexo-theme-vno" target="_blank">Vno</a>, 原创出自<a href="http://github.com/onevcat/vno" target="_blank">onevcat</a>
    </span>
</footer>
        </div>
    </div>

    

     
<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-88357723-1', 'auto');
	ga('send', 'pageview');
</script>

    
    <script>
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?a386cb01ab38fbd195ecd0b8943ecb99";
            var s = document.getElementsByTagName("script")[0]; 
            s.parentNode.insertBefore(hm, s);
        })();
    </script>



    <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
    
    </script>
    
</body>
</html>
